{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Recomendations\n",
    "\n",
    "In this project you will make an unsupervised system for music recommendations based on a song.\n",
    "\n",
    "All of the data given to you is from Spotify. For definitions of some of the columns see https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features. The dataset has been partially processed to remove very unusual artists.\n",
    "\n",
    "You will need to go through the entire machine learning process but for unsupervised learning (including big picture, exploration, ...). You will *not* split off a training and testing set. Also, you will not use cross-validation (see an example in a class notebook on how to convince `GridSearchCV` to not use CV). The ultimate goal is to be able to find which 'group' an audio track (which can be music, audio books, or other recordings) belongs to. We also want to make sure that none of the clusters are too small (so that if we ask for a related song, there is a significant amount of variability in the song that we get).\n",
    "\n",
    "To help with making good clusters, you should form clusters for the artists and then integrate that information into each track's data. As with all preprocessing, you should try with and without this step (along with different clusterings of artists).\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Artists Feature\n",
    "\n",
    "The feature `artists` is actually a series of Python lists. To use them is a bit hard, but here are a few examples:\n",
    "  * Get the length of the list for each row:  \n",
    "    `data.artists.str.len()`\n",
    "  * Get the first element of the list for each row:  \n",
    "    `data.artists.apply(lambda artists:artists[0])`\n",
    "  * Get the second element of the list for each row, or None if there is only one:   \n",
    "    `data.artists.apply(lambda artists:artists[1] if len(artists) > 1 else None)`\n",
    "  * To transform each artist in the list based on a dictionary named `trans` that has keys that are artists:  \n",
    "    `data.artists.apply(lambda artists:(trans[artist] for artist in artists))`\n",
    "  * To get the most common value from each list (once converted into something like numbers):  \n",
    "    `data.artists.apply(statistics.mode)`\n",
    "  * To make the list into multiple columns (filled with `None` for rows with fewer than max artists):  \n",
    "    `pd.DataFrame({f'artists_{i}':data.artists.apply(lambda artists:artists[i] if len(artists) > i else None) for i in range(data.artists.str.len().max())})`\n",
    "\n",
    "Also, other methods like `explode()` may be useful.\n",
    "\n",
    "\n",
    "### Scoring\n",
    "\n",
    "To be able to evaluate our model, we will need to use a custom scorer that can be used with `GridSearchCV` and `RandomSearchCV` that prefers clusters that contain pairs used by people in their personal playlists. This playlist data is in the `pairs` data. **This dataset must only be used for scoring.** Additionally, the scorer greatly penalizes having clusters that have less than 200 songs.\n",
    "\n",
    "An example of using it is like:\n",
    "\n",
    "```python\n",
    "GridSearchCV(..., scoring=MusicScorer(data, pairs), ...)\n",
    "```\n",
    "\n",
    "where `data` and `pairs` are the full dataset and the pairs dataset from `load_data()`.\n",
    "\n",
    "\n",
    "### Manual Testing\n",
    "\n",
    "The function `recommendations()` can be used to perform manual testing. It can be called like:\n",
    "\n",
    "```python\n",
    "recommendations(data, clusters, [\"50woGYhAqV3KXvO1LG4zLg\", \"6pmuu4qSz2WrtGkBjUfyuz\", \"3dmqIB2Qxe2XZobw9gXxJ6\"])\n",
    "```\n",
    "\n",
    "where `data` is a `DataFrame` of all of the tracks (minimally the track ids) and `clusters` is a sequence of cluster numbers that line up with `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Returns the track data, the artist data, and which pairs of tracks show up\n",
    "    together in playlists. The pairs dataset is extremely large but used as a\n",
    "    sparse matrix. It cannot be directly used.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv('data.csv', converters={'artists':ast.literal_eval})\n",
    "    artists = pd.read_csv('artists.csv')\n",
    "    tracks = np.unique(data.id)\n",
    "    pairs = pd.DataFrame.sparse.from_spmatrix(scipy.sparse.load_npz('track_pairs.npz'), index=tracks, columns=tracks)\n",
    "    return data, artists, pairs\n",
    "\n",
    "\n",
    "def music_score(data, clusters, pairs):\n",
    "    \"\"\"\n",
    "    Scores a set of clusters based on the track data, the clusters they are assigned to,\n",
    "    and the pairs data.\n",
    "    \"\"\"\n",
    "    summation = 0\n",
    "    n_clusters = clusters.max()\n",
    "    for i in range(n_clusters):\n",
    "        tracks = data.id[clusters == i]\n",
    "        sub = pairs.loc[tracks, tracks]\n",
    "        denominator = len(tracks)\n",
    "        if denominator < 200:\n",
    "            denominator = (200*200*200*200)/(denominator*denominator*denominator)\n",
    "        summation += sub.values.sum() / denominator\n",
    "    return summation / n_clusters\n",
    "\n",
    "\n",
    "class MusicScorer:\n",
    "    \"\"\"\n",
    "    This is the actual scorer 'function' to use with `GridSearchCV`.\n",
    "    It is used like:\n",
    "\n",
    "    GridSearchCV(..., scoring=MusicScorer(data, pairs), ...)\n",
    "\n",
    "    where `data` and `pairs` are the full dataset and the pairs\n",
    "    dataset from `load_data()`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, pairs):\n",
    "        self.data = data\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __call__(self, estimator, X, y=None):\n",
    "        # Get the cluster labels\n",
    "        if hasattr(estimator, 'labels_'):\n",
    "            labels = estimator.labels_\n",
    "        elif hasattr(estimator, 'predict'):\n",
    "            labels = estimator.predict(X)\n",
    "        else:\n",
    "            labels = estimator.fit_predict(X)\n",
    "\n",
    "        # Compute the score\n",
    "        return music_score(self.data, labels, self.pairs)\n",
    "\n",
    "\n",
    "def recommendations(all_tracks, clusters, tracks, recommendations_per=5):\n",
    "    \"\"\"\n",
    "    Given the complete data set (`all_tracks` is a DataFrame of track data or is\n",
    "    a series/array of track ids) along with the `clusters` they belong to (a\n",
    "    series/array of cluster numbers), lookup the given tracks (by their ids only),\n",
    "    and return the number of recommendations per each of those tracks.\n",
    "\n",
    "    If this is given a single track (as a string), this will return a single set\n",
    "    of rows from `all_tracks`. If given a list of tracks (as a list of strings),\n",
    "    this will return a list of sets of rows from `all_tracks`. The number of rows\n",
    "    in each set is based on recommendations_per.\n",
    "    \"\"\"\n",
    "    # force types\n",
    "    full_data = len(all_tracks.shape) == 2\n",
    "    all_tracks = pd.DataFrame(all_tracks) if full_data else pd.Series(all_tracks)\n",
    "    all_tracks = all_tracks.reset_index(drop=True)\n",
    "    clusters = pd.Series(clusters).reset_index(drop=True)\n",
    "    single = isinstance(tracks, str)\n",
    "    if single: tracks = [tracks]\n",
    "\n",
    "    # get each track's cluster\n",
    "    matches = all_tracks.id.isin(tracks) if full_data else all_tracks.isin(tracks)\n",
    "    cluster_nums = clusters[matches]\n",
    "\n",
    "    # sample from each cluster\n",
    "    if single:\n",
    "        return all_tracks.loc[clusters[clusters == cluster_nums.iloc[0]].sample(recommendations_per).index]\n",
    "    return [all_tracks.loc[clusters[clusters == cluster].sample(recommendations_per).index] for cluster in cluster_nums]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af3f3f87da3ed6054cfd6162ab209d39be2005a8ae75cb4bca1364029f78b035"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
